{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# All Credit to https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/\n",
    "\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# Get the pokedex entries file\n",
    "pokedex_entries_file = open(\"dataset/pokedex_entries.json\", 'r')\n",
    "\n",
    "# Convert the pokedex entries into a dictionary\n",
    "pokemon_entries_dictionary = json.load(pokedex_entries_file)\n",
    "\n",
    "# Convert the dictionary into a list\n",
    "pokedex_entries = []\n",
    "\n",
    "# For every Pokemon...\n",
    "for pokemon, entries in pokemon_entries_dictionary.items():\n",
    "\n",
    "    # For every Entry...\n",
    "    for entry in entries:\n",
    "\n",
    "        # Add it to the list\n",
    "        pokedex_entries.append(entry.lower())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# Create sequences of length 5 tokens\n",
    "def create_seq(text, seq_len=5):\n",
    "\n",
    "    # Create a list of sequences\n",
    "    sequences = []\n",
    "\n",
    "    # If the number of tokens in 'text' is greater than 5\n",
    "    if len(text.split()) > seq_len:\n",
    "\n",
    "        # For every token\n",
    "        for i in range(seq_len, len(text.split())):\n",
    "\n",
    "            # Select a sequence of tokens\n",
    "            seq = text.split()[i-seq_len:i+1]\n",
    "\n",
    "            # Add the token to the list\n",
    "            sequences.append(\" \".join(seq))\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    # if the number of tokens in 'text' is less than or equal to 5\n",
    "    else:\n",
    "\n",
    "      return [text]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "58466"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sequence for each pokedex entry\n",
    "seqs = [create_seq(i) for i in pokedex_entries]\n",
    "\n",
    "# Merge the list of all sequences\n",
    "seqs = sum(seqs, [])\n",
    "\n",
    "# Count the sequences\n",
    "len(seqs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# Create the inputs (x) and targets (y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "  x.append(\" \".join(s.split()[:-1]))\n",
    "  y.append(\" \".join(s.split()[1:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# Create a mapping from integers to tokens\n",
    "int2token = {}\n",
    "cnt = 0\n",
    "\n",
    "for w in set(\" \".join(pokedex_entries).split()):\n",
    "  int2token[cnt] = w\n",
    "  cnt+= 1\n",
    "\n",
    "# Create a token to integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}\n",
    "\n",
    "# Find the size of our vocabulary\n",
    "vocab_size = len(int2token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# Convert a list of words into their integer representations\n",
    "def get_integer_seq(seq):\n",
    "  return [token2int[w] for w in seq.split()]\n",
    "\n",
    "# Convert the input and target sequences into their integer representations\n",
    "x_int = np.array([get_integer_seq(i) for i in x])\n",
    "y_int = np.array([get_integer_seq(i) for i in y])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Get a set of batches from the test and training sets\n",
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "\n",
    "    # The previous batch end\n",
    "    prv = 0\n",
    "\n",
    "    # Create the batches of size 'batch size'\n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "      x = arr_x[prv:n,:]\n",
    "      y = arr_y[prv:n,:]\n",
    "      prv = n\n",
    "      yield x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 91,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\" Forward pass through the network.\n",
    "            These inputs are x, and the hidden/cell state `hidden`. \"\"\"\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)\n",
    "\n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "\n",
    "        #out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" initializes hidden state \"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "\n",
    "        # if GPU is not available\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "WordLSTM(\n  (emb_layer): Embedding(5569, 200)\n  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=5569, bias=True)\n)"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "net = WordLSTM()\n",
    "\n",
    "# Use GPU\n",
    "net.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "\n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # push model to GPU\n",
    "    net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter+= 1\n",
    "\n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            # push tensors to GPU\n",
    "            inputs, targets = inputs.cuda(), targets.cuda().to(torch.int64)\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            # Update weights\n",
    "            opt.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "\n",
    "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 256...\n",
      "Epoch: 1/20... Step: 512...\n",
      "Epoch: 1/20... Step: 768...\n",
      "Epoch: 1/20... Step: 1024...\n",
      "Epoch: 1/20... Step: 1280...\n",
      "Epoch: 1/20... Step: 1536...\n",
      "Epoch: 1/20... Step: 1792...\n",
      "Epoch: 2/20... Step: 2048...\n",
      "Epoch: 2/20... Step: 2304...\n",
      "Epoch: 2/20... Step: 2560...\n",
      "Epoch: 2/20... Step: 2816...\n",
      "Epoch: 2/20... Step: 3072...\n",
      "Epoch: 2/20... Step: 3328...\n",
      "Epoch: 2/20... Step: 3584...\n",
      "Epoch: 3/20... Step: 3840...\n",
      "Epoch: 3/20... Step: 4096...\n",
      "Epoch: 3/20... Step: 4352...\n",
      "Epoch: 3/20... Step: 4608...\n",
      "Epoch: 3/20... Step: 4864...\n",
      "Epoch: 3/20... Step: 5120...\n",
      "Epoch: 3/20... Step: 5376...\n",
      "Epoch: 4/20... Step: 5632...\n",
      "Epoch: 4/20... Step: 5888...\n",
      "Epoch: 4/20... Step: 6144...\n",
      "Epoch: 4/20... Step: 6400...\n",
      "Epoch: 4/20... Step: 6656...\n",
      "Epoch: 4/20... Step: 6912...\n",
      "Epoch: 4/20... Step: 7168...\n",
      "Epoch: 5/20... Step: 7424...\n",
      "Epoch: 5/20... Step: 7680...\n",
      "Epoch: 5/20... Step: 7936...\n",
      "Epoch: 5/20... Step: 8192...\n",
      "Epoch: 5/20... Step: 8448...\n",
      "Epoch: 5/20... Step: 8704...\n",
      "Epoch: 5/20... Step: 8960...\n",
      "Epoch: 6/20... Step: 9216...\n",
      "Epoch: 6/20... Step: 9472...\n",
      "Epoch: 6/20... Step: 9728...\n",
      "Epoch: 6/20... Step: 9984...\n",
      "Epoch: 6/20... Step: 10240...\n",
      "Epoch: 6/20... Step: 10496...\n",
      "Epoch: 6/20... Step: 10752...\n",
      "Epoch: 7/20... Step: 11008...\n",
      "Epoch: 7/20... Step: 11264...\n",
      "Epoch: 7/20... Step: 11520...\n",
      "Epoch: 7/20... Step: 11776...\n",
      "Epoch: 7/20... Step: 12032...\n",
      "Epoch: 7/20... Step: 12288...\n",
      "Epoch: 7/20... Step: 12544...\n",
      "Epoch: 8/20... Step: 12800...\n",
      "Epoch: 8/20... Step: 13056...\n",
      "Epoch: 8/20... Step: 13312...\n",
      "Epoch: 8/20... Step: 13568...\n",
      "Epoch: 8/20... Step: 13824...\n",
      "Epoch: 8/20... Step: 14080...\n",
      "Epoch: 8/20... Step: 14336...\n",
      "Epoch: 8/20... Step: 14592...\n",
      "Epoch: 9/20... Step: 14848...\n",
      "Epoch: 9/20... Step: 15104...\n",
      "Epoch: 9/20... Step: 15360...\n",
      "Epoch: 9/20... Step: 15616...\n",
      "Epoch: 9/20... Step: 15872...\n",
      "Epoch: 9/20... Step: 16128...\n",
      "Epoch: 9/20... Step: 16384...\n",
      "Epoch: 10/20... Step: 16640...\n",
      "Epoch: 10/20... Step: 16896...\n",
      "Epoch: 10/20... Step: 17152...\n",
      "Epoch: 10/20... Step: 17408...\n",
      "Epoch: 10/20... Step: 17664...\n",
      "Epoch: 10/20... Step: 17920...\n",
      "Epoch: 10/20... Step: 18176...\n",
      "Epoch: 11/20... Step: 18432...\n",
      "Epoch: 11/20... Step: 18688...\n",
      "Epoch: 11/20... Step: 18944...\n",
      "Epoch: 11/20... Step: 19200...\n",
      "Epoch: 11/20... Step: 19456...\n",
      "Epoch: 11/20... Step: 19712...\n",
      "Epoch: 11/20... Step: 19968...\n",
      "Epoch: 12/20... Step: 20224...\n",
      "Epoch: 12/20... Step: 20480...\n",
      "Epoch: 12/20... Step: 20736...\n",
      "Epoch: 12/20... Step: 20992...\n",
      "Epoch: 12/20... Step: 21248...\n",
      "Epoch: 12/20... Step: 21504...\n",
      "Epoch: 12/20... Step: 21760...\n",
      "Epoch: 13/20... Step: 22016...\n",
      "Epoch: 13/20... Step: 22272...\n",
      "Epoch: 13/20... Step: 22528...\n",
      "Epoch: 13/20... Step: 22784...\n",
      "Epoch: 13/20... Step: 23040...\n",
      "Epoch: 13/20... Step: 23296...\n",
      "Epoch: 13/20... Step: 23552...\n",
      "Epoch: 14/20... Step: 23808...\n",
      "Epoch: 14/20... Step: 24064...\n",
      "Epoch: 14/20... Step: 24320...\n",
      "Epoch: 14/20... Step: 24576...\n",
      "Epoch: 14/20... Step: 24832...\n",
      "Epoch: 14/20... Step: 25088...\n",
      "Epoch: 14/20... Step: 25344...\n",
      "Epoch: 15/20... Step: 25600...\n",
      "Epoch: 15/20... Step: 25856...\n",
      "Epoch: 15/20... Step: 26112...\n",
      "Epoch: 15/20... Step: 26368...\n",
      "Epoch: 15/20... Step: 26624...\n",
      "Epoch: 15/20... Step: 26880...\n",
      "Epoch: 15/20... Step: 27136...\n",
      "Epoch: 15/20... Step: 27392...\n",
      "Epoch: 16/20... Step: 27648...\n",
      "Epoch: 16/20... Step: 27904...\n",
      "Epoch: 16/20... Step: 28160...\n",
      "Epoch: 16/20... Step: 28416...\n",
      "Epoch: 16/20... Step: 28672...\n",
      "Epoch: 16/20... Step: 28928...\n",
      "Epoch: 16/20... Step: 29184...\n",
      "Epoch: 17/20... Step: 29440...\n",
      "Epoch: 17/20... Step: 29696...\n",
      "Epoch: 17/20... Step: 29952...\n",
      "Epoch: 17/20... Step: 30208...\n",
      "Epoch: 17/20... Step: 30464...\n",
      "Epoch: 17/20... Step: 30720...\n",
      "Epoch: 17/20... Step: 30976...\n",
      "Epoch: 18/20... Step: 31232...\n",
      "Epoch: 18/20... Step: 31488...\n",
      "Epoch: 18/20... Step: 31744...\n",
      "Epoch: 18/20... Step: 32000...\n",
      "Epoch: 18/20... Step: 32256...\n",
      "Epoch: 18/20... Step: 32512...\n",
      "Epoch: 18/20... Step: 32768...\n",
      "Epoch: 19/20... Step: 33024...\n",
      "Epoch: 19/20... Step: 33280...\n",
      "Epoch: 19/20... Step: 33536...\n",
      "Epoch: 19/20... Step: 33792...\n",
      "Epoch: 19/20... Step: 34048...\n",
      "Epoch: 19/20... Step: 34304...\n",
      "Epoch: 19/20... Step: 34560...\n",
      "Epoch: 20/20... Step: 34816...\n",
      "Epoch: 20/20... Step: 35072...\n",
      "Epoch: 20/20... Step: 35328...\n",
      "Epoch: 20/20... Step: 35584...\n",
      "Epoch: 20/20... Step: 35840...\n",
      "Epoch: 20/20... Step: 36096...\n",
      "Epoch: 20/20... Step: 36352...\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(net, batch_size = 32, epochs=20, print_every=256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "# Predict the next token\n",
    "def predict(net, tkn, h=None):\n",
    "\n",
    "  # tensor inputs\n",
    "  x = np.array([[token2int[tkn]]])\n",
    "  inputs = torch.from_numpy(x)\n",
    "\n",
    "  # push to GPU\n",
    "  inputs = inputs.cuda()\n",
    "\n",
    "  # detach hidden state from history\n",
    "  h = tuple([each.data for each in h])\n",
    "\n",
    "  # get the output of the model\n",
    "  out, h = net(inputs, h)\n",
    "\n",
    "  # get the token probabilities\n",
    "  p = F.softmax(out, dim=1).data\n",
    "\n",
    "  p = p.cpu()\n",
    "\n",
    "  p = p.numpy()\n",
    "  p = p.reshape(p.shape[1],)\n",
    "\n",
    "  # get indices of top 3 values\n",
    "  top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "  # randomly select one of the three indices\n",
    "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return int2token[sampled_token_index], h\n",
    "\n",
    "\n",
    "# function to generate text\n",
    "def sample(net, size, prime='it is'):\n",
    "\n",
    "    # push to GPU\n",
    "    net.cuda()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "      token, h = predict(net, t, h)\n",
    "\n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mew is able to live in huge colonies even if you sleep in the morning the frequency of the\n",
      "mew is able to live in thunderclouds it freely finishes it with ultrasonic waves to identify for clouds as\n",
      "mew is able to control minds this legendary cries to inflate them in thunderclouds it freely teleports in the\n",
      "mew is able to live in thunderclouds its body to check its ears are made by the ultrasonic cries\n",
      "mew is able to appear as a pet it is stricken with a dark daylight unmoving by flapping it\n",
      "mew is able to live 10000 name in caves during the day and location using reflections of the skies\n",
      "mew is able to appear from thunderclouds this pokemon can expand a legendary used that that has a texture\n",
      "mew is able to appear as it eats the enemy in caves during the day it stays with a\n",
      "mew is able to control prey and inflates its enemy it emits ultrasonic hours its mouth it uses its\n",
      "mew is able to live in huge places they wont see the victim in a dark day because exposure\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(sample(net, 15, prime=\"mew is able to \"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-b7494abd",
   "language": "python",
   "display_name": "PyCharm (Chatot)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}